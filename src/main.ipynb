{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt\n",
    "!pip install 'transformers[torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers  import  BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the datasets\n",
    "dataset = load_dataset('json', data_files={'train': 'datasets/train.json', 'dev': 'datasets/dev.json', 'test': 'datasets/test.json'})\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Define the new preprocess function\n",
    "def preprocess_data(examples):\n",
    "    # Tokenizing the questions\n",
    "    tokenized_inputs = tokenizer(examples['question'], padding='max_length', truncation=True)\n",
    "    \n",
    "    # Create a copy of the input_ids\n",
    "    labels = [ids.copy() for ids in tokenized_inputs['input_ids']]\n",
    "\n",
    "    # Identify the position of the masked token (assuming it's already there)\n",
    "    mask_positions = [[i for i, id in enumerate(ids) if id == tokenizer.mask_token_id] for ids in tokenized_inputs['input_ids']]\n",
    "\n",
    "    # Set the labels for non-masked positions to -100\n",
    "    for ids, mask_pos in zip(labels, mask_positions):\n",
    "        ids[:] = [-100 if i not in mask_pos else id for i, id in enumerate(ids)]\n",
    "    \n",
    "    # Adding the labels to the tokenized inputs\n",
    "    tokenized_inputs['labels'] = labels\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply the new preprocess function to the datasets\n",
    "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
    "mask_token = tokenizer.mask_token\n",
    "# Define the model with the correct number of labels\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8192,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['dev']\n",
    "\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
